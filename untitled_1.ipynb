{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "Using a list of all shakespear work as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-29 20:06:11--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.26MB/s    in 0.5s    \n",
      "\n",
      "2024-10-29 20:06:12 (2.26 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading the shakespear input dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and inspect dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters is 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters is \" + str(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# all unique character in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43, 2]\n",
      "hii there!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers \n",
    "stoi = { ch:i for i, ch in enumerate(chars)}\n",
    "itos = { i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]             # string -> integer list\n",
    "decode = lambda l: ''.join([itos[i] for i in l])    # integer list -> string \n",
    "\n",
    "print(encode(\"hii there!\"))\n",
    "print(decode(encode(\"hii there!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "typically use sub word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode entire dataset into a torch.tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 character from earlier will look like this to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train validation split \n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_len = 8 \n",
    "train_data[:context_len + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the reason we use a context length is not only for efficiency purposes, it's also to allow the model to be familair with context from 1 up to context length, so the model can generate with as little as 1 context up till context length, the we truncate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[39, 49,  1, 42, 43, 39, 50, 47],\n",
      "        [56,  1, 53, 61, 52,  1, 56, 39],\n",
      "        [47, 52, 43,  8,  0, 20, 39, 58],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[49,  1, 42, 43, 39, 50, 47, 52],\n",
      "        [ 1, 53, 61, 52,  1, 56, 39, 58],\n",
      "        [52, 43,  8,  0, 20, 39, 58, 46],\n",
      "        [56,  1, 58, 46, 39, 58,  1, 46]])\n",
      "---\n",
      "when input is [39], target is: 49\n",
      "when input is [39, 49], target is: 1\n",
      "when input is [39, 49, 1], target is: 42\n",
      "when input is [39, 49, 1, 42], target is: 43\n",
      "when input is [39, 49, 1, 42, 43], target is: 39\n",
      "when input is [39, 49, 1, 42, 43, 39], target is: 50\n",
      "when input is [39, 49, 1, 42, 43, 39, 50], target is: 47\n",
      "when input is [39, 49, 1, 42, 43, 39, 50, 47], target is: 52\n",
      "when input is [56], target is: 1\n",
      "when input is [56, 1], target is: 53\n",
      "when input is [56, 1, 53], target is: 61\n",
      "when input is [56, 1, 53, 61], target is: 52\n",
      "when input is [56, 1, 53, 61, 52], target is: 1\n",
      "when input is [56, 1, 53, 61, 52, 1], target is: 56\n",
      "when input is [56, 1, 53, 61, 52, 1, 56], target is: 39\n",
      "when input is [56, 1, 53, 61, 52, 1, 56, 39], target is: 58\n",
      "when input is [47], target is: 52\n",
      "when input is [47, 52], target is: 43\n",
      "when input is [47, 52, 43], target is: 8\n",
      "when input is [47, 52, 43, 8], target is: 0\n",
      "when input is [47, 52, 43, 8, 0], target is: 20\n",
      "when input is [47, 52, 43, 8, 0, 20], target is: 39\n",
      "when input is [47, 52, 43, 8, 0, 20, 39], target is: 58\n",
      "when input is [47, 52, 43, 8, 0, 20, 39, 58], target is: 46\n",
      "when input is [53], target is: 56\n",
      "when input is [53, 56], target is: 1\n",
      "when input is [53, 56, 1], target is: 58\n",
      "when input is [53, 56, 1, 58], target is: 46\n",
      "when input is [53, 56, 1, 58, 46], target is: 39\n",
      "when input is [53, 56, 1, 58, 46, 39], target is: 58\n",
      "when input is [53, 56, 1, 58, 46, 39, 58], target is: 1\n",
      "when input is [53, 56, 1, 58, 46, 39, 58, 1], target is: 46\n"
     ]
    }
   ],
   "source": [
    "# Doing batching for efficienct purposes\n",
    "torch.manual_seed(420)\n",
    "batch_size = 4      # How many independent sequence we process in parallel \n",
    "context_len = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate small batch of data for x and y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_len+1]for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets')    # y comes in at the data for measuring loss\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('---')\n",
    "\n",
    "for b in range(batch_size):         # batch dimension\n",
    "    for t in range(context_len):    # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()}, target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.9098, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "'EOfeePirXuMEwcH,!.OXFCVMwvXCVrCDxpfgWb.H;.,H-$Skb:UHhma!'UmWXJqJy$RYb.tnJ$AJQ kgI$WaDRk&FVfxzvlnNUK\n",
      "the length of the generated is: 101\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(420)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly read off the logits for the next token from a look up table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # the embedding table is vocab_size x vocab size, and we're sending in the index (column) and getting the embedding (row)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C), score for next token in a sequence\n",
    "        if targets is None: \n",
    "            loss = None \n",
    "        else: \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indicies in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the prediction \n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step \n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities \n",
    "            probs = F.softmax(logits, dim = -1) # (B, C)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss) \n",
    "\n",
    "print(decode(m.generate(torch.zeros((1,1), dtype = torch.long), max_new_tokens=100)[0].tolist())) # generate from the token 0\n",
    "print(\"the length of the generated is:\", len(decode(m.generate(torch.zeros((1,1), dtype = torch.long), max_new_tokens=100)[0].tolist()))) # 101 words, since we're printing initial token + targets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "right now it's generating garbage since we didn't train the model and it's only guessing randomly. As of right now, we're feeding all the words before context as context to guess, but a bigram model only uses the current word to predict the next, we design so to make it more adaptable later when implementing/ taking context from more characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.471271514892578\n"
     ]
    }
   ],
   "source": [
    "# simple training loop\n",
    "batch_size = 32 \n",
    "for steps in range(10000):\n",
    "\n",
    "    # sample a batch of data \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)   # zeroing out gradient from previous step\n",
    "    loss.backward()                         # getting grad of all parameter \n",
    "    optimizer.step()                        # use gradient to update parameter\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'d weand os yos mesefrem, se my dobumyower of R ther th y inmegep:\n",
      "WAn bowancrnk blind ng; w. enkitato w theremyomumbene mpirsthadond reroroulkisichau pte sodumut a heaned.\n",
      "BHUS:\n",
      "\n",
      "The'lowhore IUTo thitft and ong\n",
      "Yere.\n",
      "Ans; Clow--ounithanofon ke fthofly w the, m iloore bathe nb! to cod kipo s sene angharein maves ir; e tovee al ENeveed se.\n",
      "Whayowarere odovirowiethe t\n",
      "T:\n",
      "HAue y? be;\n",
      "s r\n",
      "PUCHELAn wherilode, ir aut!\n",
      "Sckls ts.\n",
      "SI fal w-lleay S mithootenerove ller oure fid wagbrt th, kim ongarthe latimak bes bepoid yses, bot theou,\n",
      "Thef-CHenool ne in b bath a weblleg ir iling s, t he the ty s bouspss cit DUpereare ban mshe, ipp itchon! dingoure ar, thcot f, pen lovin CELANotloongothisas I,\n",
      "\n",
      "Y m shido ineles, ltarvenewis bore d berathavedinfain th t\n",
      "METy t IS:\n",
      "MAsmavamuske tit: k,\n",
      "Hailenotondavee: tho ngepheryo, myores, th y are TIS:\n",
      "Thatheve:\n",
      "emank f thomathenorelt t arin ortrere qugs ty!\n",
      "AUESovee hen\n",
      "de ne Whesthave atof hior o th,\n",
      "De\n",
      "SIOLer, couconent howhy.\n",
      "\n",
      "N ke I s t d onth y a publ?\n",
      "I \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1,1), dtype = torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematicak trick in self-attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "right now T represents context length dimension, B represents batch size (introduced to make sure all gpus are fired during training), C represents vocab_size? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(420)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1 \n",
    "# we want x[b, t] = mean_{i<=x} x[b, i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1] # (T, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0070,  0.5044],\n",
       "        [ 0.6704, -0.3829],\n",
       "        [ 0.0302,  0.3826],\n",
       "        [-0.5131,  0.7104],\n",
       "        [ 1.8092,  0.4352],\n",
       "        [ 2.6453,  0.2654],\n",
       "        [ 0.9235, -0.4376],\n",
       "        [ 2.0182,  1.3498]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0070,  0.5044],\n",
       "        [ 0.3317,  0.0608],\n",
       "        [ 0.2312,  0.1681],\n",
       "        [ 0.0451,  0.3037],\n",
       "        [ 0.3979,  0.3300],\n",
       "        [ 0.7725,  0.3192],\n",
       "        [ 0.7941,  0.2111],\n",
       "        [ 0.9471,  0.3534]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we're adding in context by adding up all previous + current vectors -> average them up, but this approach losses a lot of information on the way so it's not good, and current method is not efficient, so we'll introduce matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "wei = torch.tril(torch.ones(T, T))      # create a bottom triangle matrix, used to achieve cumulation of previous inputs \n",
    "wei = wei/ wei.sum(1, keepdim=True)     # normalise inputs to be between 1 for efficiency sake \n",
    "xbow2 = wei @ x                         # matrix multiplication (T, T) @ (B, T, C) -> (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "torch.allclose(xbow2, xbow)             # compare x bag of words & x bag of words 2, should be same since method is same, except improvment in efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention\n",
    "torch.manual_seed(420)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Single head self attention\n",
    "head_size = 16 \n",
    "key = nn.Linear(C, head_size, bias=False)   # what a token contains (I am a consonent, vowel, etc.)\n",
    "query = nn.Linear(C, head_size, bias=False) # what a token is looking for (consenet, vowel, etc)\n",
    "value = nn.Linear(C, head_size, bias=False) # what a token will communicate to you \n",
    "k = key(x)      # (B, T, 16)\n",
    "q = query(x)    # (B, T, 16)\n",
    "# For elements with matching/ high afinity of query and key, the dot product result with be higher (the vector will move further in a direction)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T) ; This is where the data starts to talk with each other, to find who are interesting. \n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # mask future information \n",
    "wei = F.softmax(wei, dim=-1)                    # improves the distribution \n",
    "\n",
    "v = value(x)\n",
    "# after our token know who they find interesting (high dot product), the value are the values that they provide. \n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: \n",
    "* Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights (in this purpose, its a graph of context_size length and all previous & current --> current )\n",
    "* There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens (unlike RNN and CNNs that inherently encode position in their structure)\n",
    "* Each example across batch dimension is process independently and never \"talk\" to each other. (using the graph example, there will be 4 pools of nodes and each doing their own things, sort of like neighbours)\n",
    "* 'Self-attention' just means that the keys and values are produced from the same source as queries. In 'cross-attention', the queries still get produced from x but the key and values come from some other external sources(e.g. an encoder module)\n",
    "* In an 'encoder' attention block, just delete the single line that does masking with `tril`, allowing all model to communicate. This block here is called a 'decoder' attention block because it has triangular masking, and it's usually used in autoregressive settings, like language modelling.\n",
    "* 'Scaled' attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and softmax will diffuse and not saturate too much (i.e., we don't want the data to like a one-hot encoding due to how softmax transforms the data in cases of data where differences is high). Illustration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "# scale with sqrt of head_size just to make sure during initialization, the data doesn't skew towards specific input\n",
    "wei = q @ k.transpose(-2, -1)  * head_size**-0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9413)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k .var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9401)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q .var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8642)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if our inputs are closer to zero, softmax will produce a fairly diffuse number.\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we our inputs are too extreme, the softmax will also reflect that by skewing towards the maximum/ highest number, causing the aggregating to be just from a single node.\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
